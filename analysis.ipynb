{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitc1b64a04908d4aa0af06e43278af8af3",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import scipy\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 6</th>\n      <th>Unnamed: 7</th>\n      <th>Unnamed: 8</th>\n      <th>Unnamed: 9</th>\n      <th>Unnamed: 10</th>\n      <th>Unnamed: 11</th>\n      <th>Unnamed: 12</th>\n      <th>Unnamed: 13</th>\n      <th>Unnamed: 14</th>\n      <th>Unnamed: 15</th>\n      <th>...</th>\n      <th>Unnamed: 16374</th>\n      <th>Unnamed: 16375</th>\n      <th>Unnamed: 16376</th>\n      <th>Unnamed: 16377</th>\n      <th>Unnamed: 16378</th>\n      <th>Unnamed: 16379</th>\n      <th>Unnamed: 16380</th>\n      <th>Unnamed: 16381</th>\n      <th>Unnamed: 16382</th>\n      <th>Unnamed: 16383</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.063630</td>\n      <td>-0.142438</td>\n      <td>0.449996</td>\n      <td>0.820350</td>\n      <td>1.071412</td>\n      <td>1.261822</td>\n      <td>-0.462493</td>\n      <td>0.724469</td>\n      <td>0.527336</td>\n      <td>-0.501534</td>\n      <td>...</td>\n      <td>-0.073761</td>\n      <td>-0.409489</td>\n      <td>1.522084</td>\n      <td>0.528014</td>\n      <td>-0.699945</td>\n      <td>0.615996</td>\n      <td>0.310785</td>\n      <td>-0.355818</td>\n      <td>-0.222007</td>\n      <td>0.203132</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.153056</td>\n      <td>-0.798246</td>\n      <td>-1.155813</td>\n      <td>-0.204897</td>\n      <td>1.050355</td>\n      <td>-0.299558</td>\n      <td>-0.222933</td>\n      <td>-1.819520</td>\n      <td>0.767132</td>\n      <td>0.393520</td>\n      <td>...</td>\n      <td>-0.124390</td>\n      <td>0.818038</td>\n      <td>1.126642</td>\n      <td>0.016784</td>\n      <td>1.032239</td>\n      <td>-0.786948</td>\n      <td>0.335472</td>\n      <td>1.147176</td>\n      <td>0.568247</td>\n      <td>0.200026</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.055316</td>\n      <td>-0.637399</td>\n      <td>-1.200148</td>\n      <td>0.175554</td>\n      <td>0.933502</td>\n      <td>-0.494045</td>\n      <td>-0.092673</td>\n      <td>-1.867703</td>\n      <td>0.664002</td>\n      <td>0.346203</td>\n      <td>...</td>\n      <td>0.254428</td>\n      <td>0.970204</td>\n      <td>-1.135148</td>\n      <td>-0.105556</td>\n      <td>0.182156</td>\n      <td>-1.181622</td>\n      <td>0.652101</td>\n      <td>-0.258536</td>\n      <td>-0.751698</td>\n      <td>-0.691265</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.943994</td>\n      <td>-0.458596</td>\n      <td>-1.492402</td>\n      <td>-0.564439</td>\n      <td>-0.644132</td>\n      <td>-0.446328</td>\n      <td>-0.905464</td>\n      <td>0.777686</td>\n      <td>-0.241621</td>\n      <td>0.284834</td>\n      <td>...</td>\n      <td>0.052561</td>\n      <td>0.407138</td>\n      <td>-0.544393</td>\n      <td>0.643058</td>\n      <td>-0.419786</td>\n      <td>-0.578819</td>\n      <td>-0.148596</td>\n      <td>1.172315</td>\n      <td>-0.593892</td>\n      <td>0.311826</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.650914</td>\n      <td>0.897160</td>\n      <td>0.968321</td>\n      <td>0.775981</td>\n      <td>0.814335</td>\n      <td>-0.562671</td>\n      <td>0.813696</td>\n      <td>-0.673415</td>\n      <td>-0.362649</td>\n      <td>-0.053585</td>\n      <td>...</td>\n      <td>1.923727</td>\n      <td>-0.464431</td>\n      <td>-0.164953</td>\n      <td>1.270190</td>\n      <td>-0.338140</td>\n      <td>-0.218062</td>\n      <td>0.066068</td>\n      <td>0.090702</td>\n      <td>0.488732</td>\n      <td>-0.018006</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>955</th>\n      <td>0.269881</td>\n      <td>-1.512857</td>\n      <td>-0.823172</td>\n      <td>-0.854453</td>\n      <td>0.061847</td>\n      <td>1.425443</td>\n      <td>-0.667509</td>\n      <td>1.182807</td>\n      <td>0.805094</td>\n      <td>0.952008</td>\n      <td>...</td>\n      <td>-1.981705</td>\n      <td>2.087464</td>\n      <td>1.408979</td>\n      <td>0.327324</td>\n      <td>-0.658321</td>\n      <td>-0.161019</td>\n      <td>0.558722</td>\n      <td>-1.216203</td>\n      <td>1.539183</td>\n      <td>1.580753</td>\n    </tr>\n    <tr>\n      <th>956</th>\n      <td>0.071536</td>\n      <td>0.477311</td>\n      <td>0.518995</td>\n      <td>-0.379994</td>\n      <td>-0.897044</td>\n      <td>-0.506778</td>\n      <td>-0.096523</td>\n      <td>-0.661873</td>\n      <td>-0.593999</td>\n      <td>-1.626915</td>\n      <td>...</td>\n      <td>-1.530072</td>\n      <td>-0.114678</td>\n      <td>-0.856914</td>\n      <td>-0.128381</td>\n      <td>-0.651918</td>\n      <td>-1.013577</td>\n      <td>-1.596719</td>\n      <td>-1.354767</td>\n      <td>-0.606125</td>\n      <td>-1.051897</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>-0.696776</td>\n      <td>1.662495</td>\n      <td>1.769843</td>\n      <td>1.679851</td>\n      <td>0.878894</td>\n      <td>-0.159759</td>\n      <td>2.579271</td>\n      <td>0.956634</td>\n      <td>0.682997</td>\n      <td>-0.432261</td>\n      <td>...</td>\n      <td>-0.482469</td>\n      <td>0.397617</td>\n      <td>0.591116</td>\n      <td>-0.088669</td>\n      <td>0.201367</td>\n      <td>0.167362</td>\n      <td>-0.021622</td>\n      <td>0.494654</td>\n      <td>-0.186531</td>\n      <td>-0.160217</td>\n    </tr>\n    <tr>\n      <th>958</th>\n      <td>-0.695812</td>\n      <td>0.049083</td>\n      <td>-2.324831</td>\n      <td>-0.935711</td>\n      <td>0.231920</td>\n      <td>-0.357998</td>\n      <td>-0.955622</td>\n      <td>-0.456385</td>\n      <td>0.670117</td>\n      <td>-0.062161</td>\n      <td>...</td>\n      <td>-0.217182</td>\n      <td>0.382917</td>\n      <td>-0.186142</td>\n      <td>-0.562966</td>\n      <td>0.163105</td>\n      <td>-0.373773</td>\n      <td>-0.607977</td>\n      <td>-1.130881</td>\n      <td>0.079659</td>\n      <td>-0.507262</td>\n    </tr>\n    <tr>\n      <th>959</th>\n      <td>0.806745</td>\n      <td>-0.997547</td>\n      <td>0.080091</td>\n      <td>0.090046</td>\n      <td>-0.554120</td>\n      <td>-0.475280</td>\n      <td>0.882142</td>\n      <td>1.803192</td>\n      <td>0.655712</td>\n      <td>0.314272</td>\n      <td>...</td>\n      <td>0.404778</td>\n      <td>-0.109092</td>\n      <td>-1.521764</td>\n      <td>0.283529</td>\n      <td>0.151098</td>\n      <td>0.245989</td>\n      <td>-0.768975</td>\n      <td>-0.414258</td>\n      <td>-0.329658</td>\n      <td>0.030153</td>\n    </tr>\n  </tbody>\n</table>\n<p>960 rows Ã— 16378 columns</p>\n</div>",
      "text/plain": "     Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  Unnamed: 10  Unnamed: 11  \\\n0      0.063630   -0.142438    0.449996    0.820350     1.071412     1.261822   \n1      0.153056   -0.798246   -1.155813   -0.204897     1.050355    -0.299558   \n2     -0.055316   -0.637399   -1.200148    0.175554     0.933502    -0.494045   \n3      0.943994   -0.458596   -1.492402   -0.564439    -0.644132    -0.446328   \n4     -0.650914    0.897160    0.968321    0.775981     0.814335    -0.562671   \n..          ...         ...         ...         ...          ...          ...   \n955    0.269881   -1.512857   -0.823172   -0.854453     0.061847     1.425443   \n956    0.071536    0.477311    0.518995   -0.379994    -0.897044    -0.506778   \n957   -0.696776    1.662495    1.769843    1.679851     0.878894    -0.159759   \n958   -0.695812    0.049083   -2.324831   -0.935711     0.231920    -0.357998   \n959    0.806745   -0.997547    0.080091    0.090046    -0.554120    -0.475280   \n\n     Unnamed: 12  Unnamed: 13  Unnamed: 14  Unnamed: 15  ...  Unnamed: 16374  \\\n0      -0.462493     0.724469     0.527336    -0.501534  ...       -0.073761   \n1      -0.222933    -1.819520     0.767132     0.393520  ...       -0.124390   \n2      -0.092673    -1.867703     0.664002     0.346203  ...        0.254428   \n3      -0.905464     0.777686    -0.241621     0.284834  ...        0.052561   \n4       0.813696    -0.673415    -0.362649    -0.053585  ...        1.923727   \n..           ...          ...          ...          ...  ...             ...   \n955    -0.667509     1.182807     0.805094     0.952008  ...       -1.981705   \n956    -0.096523    -0.661873    -0.593999    -1.626915  ...       -1.530072   \n957     2.579271     0.956634     0.682997    -0.432261  ...       -0.482469   \n958    -0.955622    -0.456385     0.670117    -0.062161  ...       -0.217182   \n959     0.882142     1.803192     0.655712     0.314272  ...        0.404778   \n\n     Unnamed: 16375  Unnamed: 16376  Unnamed: 16377  Unnamed: 16378  \\\n0         -0.409489        1.522084        0.528014       -0.699945   \n1          0.818038        1.126642        0.016784        1.032239   \n2          0.970204       -1.135148       -0.105556        0.182156   \n3          0.407138       -0.544393        0.643058       -0.419786   \n4         -0.464431       -0.164953        1.270190       -0.338140   \n..              ...             ...             ...             ...   \n955        2.087464        1.408979        0.327324       -0.658321   \n956       -0.114678       -0.856914       -0.128381       -0.651918   \n957        0.397617        0.591116       -0.088669        0.201367   \n958        0.382917       -0.186142       -0.562966        0.163105   \n959       -0.109092       -1.521764        0.283529        0.151098   \n\n     Unnamed: 16379  Unnamed: 16380  Unnamed: 16381  Unnamed: 16382  \\\n0          0.615996        0.310785       -0.355818       -0.222007   \n1         -0.786948        0.335472        1.147176        0.568247   \n2         -1.181622        0.652101       -0.258536       -0.751698   \n3         -0.578819       -0.148596        1.172315       -0.593892   \n4         -0.218062        0.066068        0.090702        0.488732   \n..              ...             ...             ...             ...   \n955       -0.161019        0.558722       -1.216203        1.539183   \n956       -1.013577       -1.596719       -1.354767       -0.606125   \n957        0.167362       -0.021622        0.494654       -0.186531   \n958       -0.373773       -0.607977       -1.130881        0.079659   \n959        0.245989       -0.768975       -0.414258       -0.329658   \n\n     Unnamed: 16383  \n0          0.203132  \n1          0.200026  \n2         -0.691265  \n3          0.311826  \n4         -0.018006  \n..              ...  \n955        1.580753  \n956       -1.051897  \n957       -0.160217  \n958       -0.507262  \n959        0.030153  \n\n[960 rows x 16378 columns]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_thomas = pd.read_excel('annotations/data_all_thomas.xlsx')\n",
    "data_labels_video_features = pd.read_csv('annotations/combined_labels_video_features.csv')\n",
    "labels = data_labels_video_features[['gold_gt_max_aro','gold_gt_max_like','gold_gt_max_val','gold_gt_min_aro','gold_gt_min_like','gold_gt_min_val']]\n",
    "labels_max = labels[['gold_gt_max_aro','gold_gt_max_like','gold_gt_max_val']]\n",
    "labels_min = labels[['gold_gt_min_aro','gold_gt_min_like','gold_gt_min_val']]\n",
    "video_features = data_labels_video_features.drop(['gold_gt_max_aro','gold_gt_max_like','gold_gt_max_val','gold_gt_min_aro','gold_gt_min_like','gold_gt_min_val'],axis=1)\n",
    "\n",
    "#Applying z normalization\n",
    "video_features.apply(stats.zscore)\n",
    "#features = list(video_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt')\n",
    "rf.fit(video_features,labels_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=50)\n",
    "tree.fit(video_features, labels_max)\n",
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')\n",
    "export_graphviz(tree, 'tree.dot', rounded = True, feature_names = features, class_names = ['0', '1','2'], filled = True)\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=400']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn.metrics import classification_report, confusion_matrix,recall_score\n",
    "from sklearn.svm import SVC\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(video_features, labels['gold_gt_min_val'], test_size=0.30)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "report = classification_report(y_test,y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv('report.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         6\n           2       0.74      0.71      0.73       173\n           3       0.60      0.64      0.62       109\n\n    accuracy                           0.67       288\n   macro avg       0.45      0.45      0.45       288\nweighted avg       0.67      0.67      0.67       288\n\n"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(video_features, labels['gold_gt_max_val'], test_size=0.30)\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots\n",
    "def save_boxplot(x,y):\n",
    "    box_data_1 = data_all_thomas[data_all_thomas[x] == 1 ].get(y)\n",
    "    box_data_2 = data_all_thomas[data_all_thomas[x] == 2 ].get(y)\n",
    "    box_data_3 = data_all_thomas[data_all_thomas[x] == 3 ].get(y)\n",
    "    box_data = [box_data_1,box_data_2,box_data_3]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(box_data)\n",
    "    ax.set_ylabel(y)\n",
    "    ax.set_xlabel(x)\n",
    "    fig.savefig('boxplots/boxplot_'+x+'_'+y+'.svg')\n",
    "\n",
    "\n",
    "def create_boxplots():\n",
    "    classifications = ['agreeableness','conscientiousness','extraversion','interview','neuroticism','openness']\n",
    "    variables = ['valence','arousal','likeability']\n",
    "    for x in variables:\n",
    "        for y in classifications:\n",
    "            save_boxplot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis statistics\n",
    "def stats(x):\n",
    "    data_class_1 = data_all_thomas[data_all_thomas[x] == 1 ].get('interview')\n",
    "    data_class_2 = data_all_thomas[data_all_thomas[x] == 2 ].get('interview')\n",
    "    data_class_3 = data_all_thomas[data_all_thomas[x] == 3 ].get('interview')\n",
    "    print('data1: mean=%.3f stdv=%.3f' % (np.mean(data_class_1), np.std(data_class_1)))\n",
    "    print('data2: mean=%.3f stdv=%.3f' % (np.mean(data_class_2), np.std(data_class_2)))\n",
    "    print('data3: mean=%.3f stdv=%.3f' % (np.mean(data_class_3), np.std(data_class_3)))    \n",
    "    stat, p = ttest_ind(data_class_2, data_class_3) \n",
    "    print(stat, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical significance\n",
    "def mannWithNeyuScore():\n",
    "    data_class_1 = data_all_thomas[data_all_thomas['likeability'] == 1 ].get('interview')\n",
    "    data_class_2 = data_all_thomas[data_all_thomas['likeability'] == 2 ].get('interview')\n",
    "    data_class_3 = data_all_thomas[data_all_thomas['likeability'] == 3 ].get('interview')\n",
    "    scipy.stats.mannwhitneyu(data_class_1, data_class_3)"
   ]
  }
 ]
}